import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# --- é…ç½® ---
BASE_MODEL_NAME = "Qwen/Qwen1.5-1.8B"
ADAPTER_PATH = "sft_qlora_dish_recommender/final_adapter"  # LoRA é€‚é…å™¨è·¯å¾„

def load_model_and_tokenizer():
    """åŠ è½½åŸºç¡€æ¨¡å‹å’Œ LoRA é€‚é…å™¨"""
    print("--- æ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ ---")
    
    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_NAME,
        trust_remote_code=True
    )
    tokenizer.pad_token = tokenizer.eos_token
    
    # åŠ è½½åŸºç¡€æ¨¡å‹
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_NAME,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )
    
    # åŠ è½½ LoRA é€‚é…å™¨
    print(f"--- æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨: {ADAPTER_PATH} ---")
    model = PeftModel.from_pretrained(base_model, ADAPTER_PATH)
    model.eval()
    
    print("--- æ¨¡å‹åŠ è½½å®Œæˆï¼---\n")
    return model, tokenizer

def generate_response(model, tokenizer, instruction, history="", max_length=512):
    """ç”Ÿæˆæ¨¡å‹å›å¤"""
    # æ„å»ºè¾“å…¥æ–‡æœ¬ï¼ˆä¸è®­ç»ƒæ—¶çš„æ ¼å¼ä¸€è‡´ï¼‰
    prompt = f"### ç”¨æˆ·æŒ‡ä»¤:\n{instruction}\n\n### å†å²èœè°±:\n{history}\n\n### æ¨¡å‹æ¨è:\n"
    
    # Tokenize
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=max_length)
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    
    # ç”Ÿæˆ
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id,
        )
    
    # è§£ç è¾“å‡º
    full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    # æå–æ¨¡å‹æ¨èéƒ¨åˆ†ï¼ˆå»æ‰ promptï¼‰
    response = full_output.split("### æ¨¡å‹æ¨è:\n")[-1].strip()
    
    return response

def interactive_chat():
    """äº¤äº’å¼å¯¹è¯"""
    model, tokenizer = load_model_and_tokenizer()
    
    print("=" * 60)
    print("ğŸ½ï¸  èœè°±æ¨èåŠ©æ‰‹å·²å¯åŠ¨ï¼")
    print("=" * 60)
    print("è¾“å…¥ä½ çš„éœ€æ±‚ï¼Œæˆ‘ä¼šä¸ºä½ æ¨èèœè°±ï¼")
    print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("è¾“å…¥ 'clear' æ¸…ç©ºå†å²è®°å½•")
    print("=" * 60)
    print()
    
    history = ""
    
    while True:
        # è·å–ç”¨æˆ·è¾“å…¥
        user_input = input("ğŸ‘¤ ä½ : ").strip()
        
        if not user_input:
            continue
        
        if user_input.lower() in ['quit', 'exit', 'é€€å‡º']:
            print("\nğŸ‘‹ å†è§ï¼")
            break
        
        if user_input.lower() in ['clear', 'æ¸…ç©º']:
            history = ""
            print("\nâœ… å†å²è®°å½•å·²æ¸…ç©º\n")
            continue
        
        # ç”Ÿæˆå›å¤
        print("\nğŸ¤– åŠ©æ‰‹æ­£åœ¨æ€è€ƒ...\n")
        response = generate_response(model, tokenizer, user_input, history)
        
        print(f"ğŸ¤– åŠ©æ‰‹: {response}\n")
        print("-" * 60)
        print()
        
        # æ›´æ–°å†å²ï¼ˆå¯é€‰ï¼šå¦‚æœæƒ³è®©æ¨¡å‹è®°ä½ä¹‹å‰çš„å¯¹è¯ï¼‰
        # history += f"{user_input}\n{response}\n"

def single_query(instruction, history=""):
    """å•æ¬¡æŸ¥è¯¢æ¨¡å¼"""
    model, tokenizer = load_model_and_tokenizer()
    response = generate_response(model, tokenizer, instruction, history)
    print(f"æŒ‡ä»¤: {instruction}")
    print(f"å†å²: {history}")
    print(f"æ¨è: {response}")
    return response

if __name__ == "__main__":
    interactive_chat()